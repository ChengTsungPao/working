{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclical Learning using Fashion_MNIST dataset\n",
    "## References:\n",
    "1. Leslie Smith, \"Cyclical Learning Rates for Training Neural Networks\", arXiv: 1506.01186\n",
    "2. https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "\n",
    "import os, sys\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "#from keras.datasets import fashion_mnist\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "#from keras.datasets import cifar10\n",
    "\n",
    "from learningratefinder import LearningRateFinder\n",
    "from clr_callback import CyclicLR\n",
    "#from model.minigooglenet import MiniGoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of class label names\n",
    "CLASSES = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "\n",
    "# define hyper-parameters\n",
    "MIN_LR = 1e-5\n",
    "MAX_LR = 1e-2\n",
    "BATCH_SIZE = 64\n",
    "STEP_SIZE = 8\n",
    "CLR_METHOD = \"triangular\"\n",
    "NUM_EPOCHS = 48\n",
    "lr_find = 1\n",
    "\n",
    "# define plot path\n",
    "LRFIND_PLOT_PATH = os.path.sep.join([\"output\", \"lrfind_plot2.png\"])\n",
    "TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_plot2.png\"])\n",
    "CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot2.png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading Fashion MNIST data...\")\n",
    "((X_train, Y_train), (X_test, Y_test)) = fashion_mnist.load_data()\n",
    "\n",
    "# Fashion MNIST images are 28x28, reshape to 32x32\n",
    "X_train = np.array([cv2.resize(x, (32, 32)) for x in X_train])\n",
    "X_test = np.array([cv2.resize(x, (32, 32)) for x in X_test])\n",
    "\n",
    "# normalization\n",
    "X_train = X_train.astype(\"float\") / 255.0\n",
    "X_test = X_test.astype(\"float\") / 255.0\n",
    "\n",
    "# reshape\n",
    "X_train = X_train.reshape((X_train.shape[0], 32, 32, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 32, 32, 1))\n",
    "\n",
    "\"\"\"One can also try cifar10 dataset\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((X_train, Y_train), (X_test, Y_test)) = cifar10.load_data()\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "# apply mean subtraction to the data\n",
    "mean = np.mean(X_train, axis=0)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\"\"\"\n",
    "\n",
    "# One-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "gen_aug = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1,conv2,linear = (3, 3, 5, 1, 'same'), (3, 3, 5, 1, 'same'), (\"\", 512, 2)\n",
    "channelsize = (36, 72)\n",
    "\n",
    "def weight(x):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(batch_input_shape=(None, channelsize[0], channelsize[1], conv1[0]), \n",
    "                            filters=conv1[1], \n",
    "                            kernel_size=conv1[2], \n",
    "                            strides=conv1[3],\n",
    "                            padding=conv1[4])\n",
    "             )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=conv2[1], \n",
    "                            kernel_size=conv2[2], \n",
    "                            strides=conv2[3],\n",
    "                            padding=conv2[4])\n",
    "             )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    return model(x)\n",
    "\n",
    "def resnet():\n",
    "    x = weight(x)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(linear[1]))\n",
    "    model.add(Dense(linear[2]))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 72, 36, 3)         228       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 72, 36, 3)         12        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 72, 36, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 72, 36, 3)         228       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 72, 36, 3)         12        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 72, 36, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3981824   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,983,330\n",
      "Trainable params: 3,983,318\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "#model = MiniGoogLeNet.build(width=32, height=32, depth=1, classes=10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MIN_LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-3f5e1f8817f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# initialize the optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] compiling model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMIN_LR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MIN_LR' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=MIN_LR, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_find' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-ca321c861026>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mlr_find\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# initialize the learning rate finder (from 1e-10 to 1e+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] finding learning rate...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearningRateFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     lrf.find(gen_aug.flow(X_train, Y_train, batch_size=BATCH_SIZE), 1e-10, 1e+1,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_find' is not defined"
     ]
    }
   ],
   "source": [
    "if lr_find > 0:\n",
    "# initialize the learning rate finder (from 1e-10 to 1e+1)\n",
    "    print(\"[INFO] finding learning rate...\")\n",
    "    lrf = LearningRateFinder(model)\n",
    "    lrf.find(gen_aug.flow(X_train, Y_train, batch_size=BATCH_SIZE), 1e-10, 1e+1,\n",
    "    stepsPerEpoch=np.ceil((len(X_train) / float(BATCH_SIZE))), batchSize=BATCH_SIZE)\n",
    "\n",
    "    # plot the loss and save the resulting plot to disk\n",
    "    lrf.plot_loss()\n",
    "    plt.savefig(LRFIND_PLOT_PATH)\n",
    "    \n",
    "    print(\"[INFO] learning rate finder complete\")\n",
    "    print(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "    #exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclical training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherwise, we have already defined a learning rate space to train\n",
    "# over, so compute the step size and initialize the cyclic learning\n",
    "# rate method\n",
    "stepSize = STEP_SIZE * (X_train.shape[0] // BATCH_SIZE)\n",
    "clr = CyclicLR(mode=CLR_METHOD, base_lr=MIN_LR, max_lr=MAX_LR, step_size=stepSize)\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(gen_aug.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_test, Y_test),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[clr],\n",
    "    verbose=1)\n",
    "\n",
    "# evaluate the network and show a classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "print(classification_report(Y_test.argmax(axis=1),\n",
    "predictions.argmax(axis=1), target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(TRAINING_PLOT_PATH)\n",
    "\n",
    "# plot the learning rate history\n",
    "N = np.arange(0, len(clr.history[\"lr\"]))\n",
    "plt.figure()\n",
    "plt.plot(N, clr.history[\"lr\"])\n",
    "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.savefig(CLR_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
