{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers : Fundamental blocks of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear(in_features,out_features,bias)\n",
    "\n",
    "define the dimension of the input layer and output layer(weight dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5247,  0.1467,  0.5587, -0.3075, -0.3586]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer = Linear(in_features=10,out_features=5,bias=True)\n",
    "inp = Variable(torch.randn(1,10))\n",
    "myLayer = Linear(in_features=10,out_features=5,bias=True) \n",
    "myLayer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2687,  0.1919, -0.1918,  0.1634,  0.1679, -0.2726,  0.1210,  0.0212,\n",
       "          0.2448, -0.1230],\n",
       "        [ 0.2899, -0.1558, -0.2223,  0.2081, -0.1874,  0.1337,  0.0752, -0.1109,\n",
       "          0.1968, -0.0490],\n",
       "        [ 0.0105,  0.2710, -0.2534,  0.1293, -0.1443,  0.1069,  0.1113,  0.1059,\n",
       "          0.1663, -0.0865],\n",
       "        [-0.1100, -0.3043, -0.1278,  0.1770, -0.0178, -0.0161,  0.1235, -0.2002,\n",
       "          0.0184,  0.1009],\n",
       "        [ 0.0549, -0.2641,  0.2149,  0.1767, -0.1061, -0.2948,  0.1675,  0.3118,\n",
       "          0.2775,  0.2681]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1817,  0.2312, -0.1881,  0.0167,  0.0032], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Linear layers\n",
    "\n",
    "Create the multilayer neural layer through iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0548, 0.7989]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer1 = Linear(10,5)\n",
    "myLayer2 = Linear(5,2)\n",
    "myLayer2(myLayer1(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Non-linear Activations\n",
    "\n",
    "Use different way to use activate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = Variable(torch.Tensor([[1,2,-1,-1]])) \n",
    "myRelu = ReLU()\n",
    "myRelu(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sample_data = Variable(torch.Tensor([[1,2,-1,-1]])) \n",
    "f = F.relu(sample_data) # Much simpler.\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network \n",
    "\n",
    "Create the multilayer neural layer through class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFirstNetwork(\n",
      "  (layer1): Linear(in_features=23716, out_features=3136, bias=True)\n",
      "  (layer2): Linear(in_features=3136, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyFirstNetwork(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MyFirstNetwork,self).__init__() \n",
    "        self.layer1 = nn.Linear(input_size,hidden_size) \n",
    "        self.layer2 = nn.Linear(hidden_size,output_size)\n",
    "    def layer(self):\n",
    "        first  = [self.layer1.weight.data,self.layer1.bias.data]\n",
    "        second = [self.layer2.weight.data,self.layer2.bias.data]\n",
    "        return first,second\n",
    "    def forward(self,input): \n",
    "        out = self.layer1(input) \n",
    "        out = F.relu(out)\n",
    "        out = self.layer2(out) \n",
    "        out = F.softmax(out,dim=1)\n",
    "        return out\n",
    "\n",
    "model = MyFirstNetwork(154*154,56*56,2)\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Introduction the two different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = Variable(torch.randn(3, 5), requires_grad=True) \n",
    "target = Variable(torch.randn(3, 5))\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commonly used cross-entropy(loss function) for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(true_label, prediction):\n",
    "    if true_label == 1:\n",
    "        return -log(prediction)\n",
    "    else:\n",
    "        return -log(1 - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss(x,class)=−x[class]+log(∑exp(x[j]))\n",
    "\n",
    "Introduction the \"nn.CrossEntropyLoss\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7454723119735718\n",
      "1.7454723055142498\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = Variable(torch.randn(3, 5), requires_grad=True) \n",
    "target = Variable(torch.LongTensor(3).random_(5)) \n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output.data.item())\n",
    "\n",
    "def CrossEntropyLoss(input,target):\n",
    "    sum = 0\n",
    "    for i in range(len(input)):\n",
    "        tmp = 0\n",
    "        for j in range(len(input[0])):\n",
    "            tmp+=np.e**(input[i][j])\n",
    "        sum+=-input[i][target[i]]+np.log(tmp)\n",
    "    return float(sum)/len(input)\n",
    "\n",
    "print(CrossEntropyLoss(input.data.numpy(),target.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Dataset\n",
    "\n",
    "use class to collect the some graph\n",
    "\n",
    "_init_:initialized all of the object\n",
    "\n",
    "_len_:number of information(graph)\n",
    "\n",
    "_getitem_:get the number of Kth information(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "# ### Optimizer\n",
    "class DogsAndCatsDataset(Dataset):\n",
    "    def __init__(self,root_dir,size=(154,154)):\n",
    "        self.files = glob(root_dir)\n",
    "        self.size = size\n",
    "        self.arr = np.array([[0],[1]],int)\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self,idx):\n",
    "        input = Variable(torch.from_numpy(np.asarray(Image.open(self.files[idx]).resize((self.size)).convert('L'), float).reshape(1,-1)))  \n",
    "        if(idx<1000):\n",
    "            target = Variable(torch.from_numpy(self.arr[0]))\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(self.arr[1])) \n",
    "        return input,target\n",
    "\n",
    "dataset = DogsAndCatsDataset(\"D:/program/vscode_workspace/private/data/dogs-vs-cats/classifier2/*.jpg\")\n",
    "test = DogsAndCatsDataset(\"D:/program/vscode_workspace/private/data/dogs-vs-cats/sample_test/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "  Completion ratio : 8.7%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fc932c1d0a0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "t1 = perf_counter()\n",
    "t  = t1\n",
    "for i in range(2):\n",
    "    index = 0\n",
    "    print(\"epoch\",i)\n",
    "    for input, target in dataset:\n",
    "        if torch.cuda.is_available():\n",
    "            input = Variable(input.cuda())\n",
    "            target = Variable(target.cuda())\n",
    "        else:\n",
    "            input, target = Variable(input), Variable(target)    \n",
    "        output = model(input.float()/(3*255.))\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        index+=1\n",
    "        if(perf_counter()-t > 30):\n",
    "            t = perf_counter()\n",
    "            print(\"  Completion ratio :\",str(index*100.0/2000)+\"%\")\n",
    "    ans=0\n",
    "    for j in range(10):        \n",
    "        output = test[j][0].cuda()\n",
    "        output = model(output.float())     \n",
    "        tmp = output.data.cpu().numpy()     \n",
    "        if(tmp[0][0]>tmp[0][1] and j<5):\n",
    "            ans+=1\n",
    "        if(tmp[0][0]<tmp[0][1] and j>=5):\n",
    "            ans+=1\n",
    "    print(\"  accuracy :\",str(ans*100/10.)+\"%\")\n",
    "\n",
    "        \n",
    "t2  = perf_counter()\n",
    "\n",
    "print(model)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"layer1 :\",\"\\nweight :\\n\",model.layer()[0][0].cpu().numpy(),\"\\nbias :\\n\",model.layer()[0][1].cpu().numpy())\n",
    "print(\"\")\n",
    "print(\"layer2 :\",\"\\nweight :\\n\",model.layer()[1][0].cpu().numpy(),\"\\nbias :\\n\",model.layer()[1][1].cpu().numpy())\n",
    "print(\"time :\",str((t2-t1)/60.0)+\"min\")\n",
    "#print(\"\\ntest.....\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
